{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "amino-fashion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidatorModel\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "canadian-government",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating sparkContext and sparkSession\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adult-helena",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read\n",
    "          .format(\"csv\")\n",
    "          .option('header', 'true')\n",
    "          .load(\"./datasets/training.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "intermediate-amber",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputCols for StringIndexer\n",
    "string_col = [\n",
    "    'DataSource','DataValueUnit','DataValueTypeID',\n",
    "    'QuestionID','LocationID','StratificationCategoryID1',\n",
    "    'StratificationID1','TopicID'\n",
    "]\n",
    "\n",
    "#outputCols for StringIndexer\n",
    "string_col_output = [\n",
    "    'DataSourceIndex','DataValueUnitIndex','DataValueTypeIDIndex',\n",
    "    'QuestionIDIndex','LocationIDIndex','StratificationCategoryID1Index',\n",
    "    'StratificationID1Index','TopicIDIndex'\n",
    "]\n",
    "\n",
    "#inputCols for OneHotEncoder\n",
    "string_col_encode_input = [\n",
    "    'DataSourceIndex','DataValueUnitIndex','DataValueTypeIDIndex',\n",
    "    'QuestionIDIndex','LocationIDIndex','StratificationCategoryID1Index',\n",
    "    'StratificationID1Index'\n",
    "]\n",
    "\n",
    "#outputCols for OneHotEncoder\n",
    "string_col_encoded = [\n",
    "    'DataSourceVec','DataValueUnitVec','DataValueTypeIDVec',\n",
    "    'QuestionIDVec','LocationIDVec','StratificationCategoryID1Vec',\n",
    "    'StratificationID1Vec'\n",
    "]\n",
    "\n",
    "#inputCols for VectorAssembler\n",
    "features_to_assemble = string_col_encoded + ['DataValue','LowConfidenceLimit','HighConfidenceLimit','Geo_lat','Geo_lon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "painted-indiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns  = df.columns\n",
    "\n",
    "dataset = df.select(col(columns[0]).cast('string'),\n",
    "                    col(columns[1]).cast('string'),\n",
    "                    col(columns[2]).cast('string'),\n",
    "                    col(columns[3]).cast('float'),\n",
    "                    col(columns[4]).cast('float'),\n",
    "                    col(columns[5]).cast('float'),\n",
    "                    col(columns[6]).cast('string'),\n",
    "                    col(columns[7]).cast('string'),\n",
    "                    col(columns[8]).cast('string'),\n",
    "                    col(columns[9]).cast('string'),\n",
    "                    col(columns[10]).cast('string'),\n",
    "                    col(columns[11]).cast('float'),\n",
    "                    col(columns[12]).cast('float'),\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "painted-element",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCols= string_col, outputCols=string_col_output)\n",
    "encoder = OneHotEncoder(inputCols=string_col_encode_input, outputCols=string_col_encoded)\n",
    "vectorAssembler = VectorAssembler(inputCols=features_to_assemble,\n",
    "                                  outputCol=\"features\")\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)\n",
    "scaler = StandardScaler(inputCol=\"features_norm\", outputCol=\"features_norm_scaled\")\n",
    "pipeline = Pipeline(stages=[indexer, encoder, vectorAssembler, normalizer,scaler])\n",
    "model = pipeline.fit(dataset)\n",
    "dd = model.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "facial-spectrum",
   "metadata": {},
   "outputs": [],
   "source": [
    "path= r'./SavedModels/lr_model'\n",
    "lr_model = CrossValidatorModel.read().load(path)\n",
    "lr = lr_model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "heated-robinson",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = []\n",
    "prediction = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "indirect-think",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_report(metrics,dataset) : \n",
    "    print('Accuracy : {} %'.format(metrics.evaluate(dataset,{metrics.metricName: \"accuracy\"})*100))\n",
    "    print('f1-score : {}'.format(metrics.evaluate(dataset,{metrics.metricName: \"f1\"})))\n",
    "    print('precision-score : {}'.format(metrics.evaluate(dataset,{metrics.metricName: \"precisionByLabel\"})))\n",
    "    print('recall-score : {}'.format(metrics.evaluate(dataset,{metrics.metricName: \"recallByLabel\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "convenient-export",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train(text) :\n",
    "    global dtc,prediction,label\n",
    "    if text.collect() != [] :\n",
    "        for data in text.collect() :\n",
    "            data = data.split('|')\n",
    "            feature = [(data[0]),data[1],data[2],float(data[3]),float(data[4]),float(data[5]),data[6],data[7],data[8],data[9],data[10],float(data[11]),float(data[12])]\n",
    "            #print(feature)\n",
    "            Dframe = sc.parallelize([feature]).toDF(('DataSource','DataValueUnit','DataValueTypeID','DataValue','LowConfidenceLimit','HighConfidenceLimit','TopicID','QuestionID','LocationID','StratificationCategoryID1','StratificationID1','Geo_lat','Geo_lon'))\n",
    "            #print(Dframe.collect())\n",
    "            df = model.transform(Dframe)\n",
    "            #print(df.collect())\n",
    "            tempdf = lr.transform(df)\n",
    "            \n",
    "            prediction.append(float(tempdf.toPandas().pred_lr.values[0]))\n",
    "            label.append(float(tempdf.toPandas().TopicIDIndex.values[0]))\n",
    "    else :\n",
    "        data = spark.createDataFrame(list(zip(prediction,label)),['prediction','label'])\n",
    "        metrics = MulticlassClassificationEvaluator()\n",
    "        metrics = metrics.setPredictionCol('prediction')\n",
    "        classification_report(metrics,data)\n",
    "        try :\n",
    "            ssc.stop()\n",
    "        except Py4JJavaError() as err:\n",
    "            print(err)\n",
    "            print('Streaming Stopped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "educated-expense",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 2)\n",
    "lines = ssc.socketTextStream('localhost', 9991)\n",
    "lines.foreachRDD(test_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "manual-density",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 45.83333333333333 %\n",
      "f1-score : 0.4313949938949939\n",
      "precision-score : 1.0\n",
      "recall-score : 0.5\n"
     ]
    }
   ],
   "source": [
    "ssc.start()             # Start the streaming process\n",
    "ssc.awaitTermination()  # Wait for the streaming to end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "severe-carroll",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

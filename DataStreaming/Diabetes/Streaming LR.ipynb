{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "premier-remark",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidatorModel\n",
    "from pyspark.ml import Pipeline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dress-judges",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating sparkContext and sparkSession\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "supreme-royal",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read\n",
    "          .format(\"csv\")\n",
    "          .option('header', 'true')\n",
    "          .load(\"./datasets/training.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "gross-transmission",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns  = df.columns\n",
    "\n",
    "dataset = df.select(col(columns[0]).cast('float'),\n",
    "                    col(columns[1]).cast('float'),\n",
    "                    col(columns[2]).cast('string'),\n",
    "                    col(columns[3]).cast('float'),\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cultural-stomach",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol=\"code\", outputCol=\"codeIndex\")\n",
    "encoder = OneHotEncoder(inputCol=\"codeIndex\", outputCol=\"codeVec\")\n",
    "vectorAssembler = VectorAssembler(inputCols=['value','codeVec'],\n",
    "                                  outputCol=\"features\")\n",
    "#normalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"features_norm\")\n",
    "pipeline = Pipeline(stages=[indexer, encoder, vectorAssembler, scaler])\n",
    "model = pipeline.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "sixth-thesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "path= r'./SavedModel/lr_model'\n",
    "lr_model = CrossValidatorModel.read().load(path)\n",
    "lr = lr_model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "earned-counter",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = []\n",
    "prediction = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "processed-acquisition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tp(label,prediction) :\n",
    "    '''\n",
    "    Positive class is 0\n",
    "    '''\n",
    "    total_positive_instances_indexes = label[label.values == 0.0].index\n",
    "    prediction_instances = prediction[total_positive_instances_indexes]\n",
    "    true_labels= prediction_instances[prediction_instances.values == 0.0]\n",
    "    return len(true_labels)\n",
    "\n",
    "\n",
    "def tn(label,prediction) :\n",
    "    '''\n",
    "    Negative Class is 1\n",
    "    '''\n",
    "    total_negative_instances_indexes = label[label.values == 1.0].index\n",
    "    prediction_instances = prediction[total_negative_instances_indexes]\n",
    "    true_labels= prediction_instances[prediction_instances.values == 1.0]\n",
    "    return len(true_labels)\n",
    "\n",
    "\n",
    "def fp(label,prediction) :\n",
    "\n",
    "    '''\n",
    "    Positive class is 0\n",
    "    '''\n",
    "    total_positive_instances_indexes = label[label.values == 0.0].index\n",
    "    prediction_instances = prediction[total_positive_instances_indexes]\n",
    "    true_labels= prediction_instances[prediction_instances.values == 0.0]\n",
    "    return len(prediction_instances) - len(true_labels)\n",
    "\n",
    "\n",
    "def fn(label,prediction) :\n",
    "    '''\n",
    "    Negative Class is 1\n",
    "    '''\n",
    "    total_negative_instances_indexes = label[label.values == 1.0].index\n",
    "    prediction_instances = prediction[total_negative_instances_indexes]\n",
    "    true_labels= prediction_instances[prediction_instances.values == 1.0]\n",
    "    return len(prediction_instances) - len(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "soviet-browser",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(labels,predictions) :\n",
    "    _tp = tp(labels,predictions)\n",
    "    _tn = tn(labels,predictions)\n",
    "    _fp = fp(labels,predictions)\n",
    "    _fn = fn(labels,predictions)\n",
    "    \n",
    "    return ((_tp+_tn)/(_tp+_tn+_fp+_fn))\n",
    "\n",
    "def precision(labels,predictions) : \n",
    "    _tp = tp(labels,predictions)\n",
    "    _fp = fp(labels,predictions)\n",
    "    \n",
    "    return (_tp/(_tp+_fp))\n",
    "\n",
    "def recall(labels,predictions) : \n",
    "    _tp = tp(labels,predictions)\n",
    "    _fn = fn(labels,predictions)\n",
    "    \n",
    "    return (_tp / (_tp+_fn))\n",
    "\n",
    "\n",
    "def f1_score(labels,predictions) :\n",
    "    pr = precision(labels,predictions)\n",
    "    re = recall(labels,predictions)\n",
    "    \n",
    "    return ((2*pr*re)/(pr+re))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "expanded-absolute",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train(text) :\n",
    "    #i =0\n",
    "    global lr,prediction,label\n",
    "    if text.collect() != [] :\n",
    "        for data in text.collect() :\n",
    "            data = data.split('|')\n",
    "            #print(data)\n",
    "            feature = [float(data[0]),float(data[1]),data[2],float(data[3])]\n",
    "            #print(feature)\n",
    "            #print(feature)\n",
    "            Dframe = sc.parallelize([feature]).toDF(('timestamp','value','code','Class'))\n",
    "            df = model.transform(Dframe)\n",
    "            #print('---->',x.collect())\n",
    "            #print(x.select(['features_norm']).collect())\n",
    "            tempdf = lr.transform(df)\n",
    "            #if prediction is None and label is None : \n",
    "            prediction.append(tempdf.toPandas().pred_lr.values[0])\n",
    "            label.append(tempdf.toPandas().Class.values[0])\n",
    "            #print('pred :', prediction)\n",
    "            #print('label :',label)\n",
    "            #i+=1\n",
    "            #print(i)\n",
    "    else :\n",
    "        #print('Empty!!!!')\n",
    "        try :\n",
    "            ssc.stop()\n",
    "        except Py4JJavaError() as err:\n",
    "            print(err)\n",
    "            print('Streaming Stopped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ancient-bolivia",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 2)\n",
    "lines = ssc.socketTextStream('localhost', 9995)\n",
    "lines.foreachRDD(test_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "first-handling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty!!!!\n"
     ]
    }
   ],
   "source": [
    "ssc.start()             # Start the streaming process\n",
    "ssc.awaitTermination()  # Wait for the streaming to end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "satellite-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_report(label,prediction) : \n",
    "    label_ = pd.Series(label)\n",
    "    prediction_ = pd.Series(prediction)\n",
    "    print('Accuracy : {}'.format(accuracy(label_,prediction_)))\n",
    "    print('Precision : {}'.format(precision(label_,prediction_)))\n",
    "    print('Recall : {}'.format(recall(label_,prediction_)))\n",
    "    print('f1-score : {}'.format(f1_score(label_,prediction_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "worst-brighton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 1.0\n",
      "Precision : 1.0\n",
      "Recall : 1.0\n",
      "f1-score : 1.0\n"
     ]
    }
   ],
   "source": [
    "classification_report(label,prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-seven",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
